{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learners.s_learner import S_learner\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from dataloader import load_IHDP_data\n",
    "from visualizations import plot_cates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.read_csv('jobsdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>treat</th>\n",
       "      <th>econ_hard</th>\n",
       "      <th>depress1</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>occp</th>\n",
       "      <th>marital</th>\n",
       "      <th>nonwhite</th>\n",
       "      <th>educ</th>\n",
       "      <th>income</th>\n",
       "      <th>job_seek</th>\n",
       "      <th>depress2</th>\n",
       "      <th>work1</th>\n",
       "      <th>comply</th>\n",
       "      <th>control</th>\n",
       "      <th>job_dich</th>\n",
       "      <th>job_disc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>34.167122</td>\n",
       "      <td>professionals</td>\n",
       "      <td>married</td>\n",
       "      <td>non.white1</td>\n",
       "      <td>gradwk</td>\n",
       "      <td>50k+</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>psyemp</td>\n",
       "      <td>0</td>\n",
       "      <td>treat</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0</td>\n",
       "      <td>26.101370</td>\n",
       "      <td>operatives/kindred wrks</td>\n",
       "      <td>nevmarr</td>\n",
       "      <td>white0</td>\n",
       "      <td>somcol</td>\n",
       "      <td>15t24k</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>psyemp</td>\n",
       "      <td>0</td>\n",
       "      <td>treat</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1</td>\n",
       "      <td>35.021919</td>\n",
       "      <td>operatives/kindred wrks</td>\n",
       "      <td>nevmarr</td>\n",
       "      <td>non.white1</td>\n",
       "      <td>somcol</td>\n",
       "      <td>25t39k</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>psyump</td>\n",
       "      <td>0</td>\n",
       "      <td>treat</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0</td>\n",
       "      <td>27.487671</td>\n",
       "      <td>manegerial</td>\n",
       "      <td>married</td>\n",
       "      <td>white0</td>\n",
       "      <td>bach</td>\n",
       "      <td>25t39k</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.545455</td>\n",
       "      <td>psyump</td>\n",
       "      <td>0</td>\n",
       "      <td>control</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1</td>\n",
       "      <td>31.610958</td>\n",
       "      <td>clerical/kindred</td>\n",
       "      <td>separtd</td>\n",
       "      <td>non.white1</td>\n",
       "      <td>highsc</td>\n",
       "      <td>25t39k</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.363636</td>\n",
       "      <td>psyump</td>\n",
       "      <td>1</td>\n",
       "      <td>treat</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>895</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0</td>\n",
       "      <td>38.205479</td>\n",
       "      <td>craftsmen/foremen/kindred</td>\n",
       "      <td>married</td>\n",
       "      <td>white0</td>\n",
       "      <td>highsc</td>\n",
       "      <td>25t39k</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>psyump</td>\n",
       "      <td>0</td>\n",
       "      <td>treat</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0</td>\n",
       "      <td>40.972603</td>\n",
       "      <td>professionals</td>\n",
       "      <td>married</td>\n",
       "      <td>white0</td>\n",
       "      <td>somcol</td>\n",
       "      <td>25t39k</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>psyump</td>\n",
       "      <td>1</td>\n",
       "      <td>treat</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>48.909588</td>\n",
       "      <td>professionals</td>\n",
       "      <td>married</td>\n",
       "      <td>non.white1</td>\n",
       "      <td>gradwk</td>\n",
       "      <td>50k+</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>psyump</td>\n",
       "      <td>0</td>\n",
       "      <td>control</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>25.043835</td>\n",
       "      <td>clerical/kindred</td>\n",
       "      <td>married</td>\n",
       "      <td>white0</td>\n",
       "      <td>somcol</td>\n",
       "      <td>lt15k</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.545455</td>\n",
       "      <td>psyemp</td>\n",
       "      <td>0</td>\n",
       "      <td>control</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1</td>\n",
       "      <td>30.975342</td>\n",
       "      <td>manegerial</td>\n",
       "      <td>divrcd</td>\n",
       "      <td>non.white1</td>\n",
       "      <td>bach</td>\n",
       "      <td>25t39k</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>psyemp</td>\n",
       "      <td>0</td>\n",
       "      <td>control</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>899 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  treat  econ_hard  depress1  sex        age  \\\n",
       "0             1      1       3.00      1.91    1  34.167122   \n",
       "1             2      1       3.67      1.36    0  26.101370   \n",
       "2             3      1       4.00      2.09    1  35.021919   \n",
       "3             4      0       2.33      1.45    0  27.487671   \n",
       "4             5      1       1.33      1.73    1  31.610958   \n",
       "..          ...    ...        ...       ...  ...        ...   \n",
       "894         895      1       3.00      1.36    0  38.205479   \n",
       "895         896      1       4.00      2.55    0  40.972603   \n",
       "896         897      0       5.00      1.00    1  48.909588   \n",
       "897         898      0       4.33      1.91    1  25.043835   \n",
       "898         899      0       4.00      1.36    1  30.975342   \n",
       "\n",
       "                          occp  marital    nonwhite    educ  income  job_seek  \\\n",
       "0                professionals  married  non.white1  gradwk    50k+  4.833333   \n",
       "1      operatives/kindred wrks  nevmarr      white0  somcol  15t24k  3.833333   \n",
       "2      operatives/kindred wrks  nevmarr  non.white1  somcol  25t39k  4.500000   \n",
       "3                   manegerial  married      white0    bach  25t39k  3.666667   \n",
       "4             clerical/kindred  separtd  non.white1  highsc  25t39k  2.500000   \n",
       "..                         ...      ...         ...     ...     ...       ...   \n",
       "894  craftsmen/foremen/kindred  married      white0  highsc  25t39k  4.166667   \n",
       "895              professionals  married      white0  somcol  25t39k  3.000000   \n",
       "896              professionals  married  non.white1  gradwk    50k+  5.000000   \n",
       "897           clerical/kindred  married      white0  somcol   lt15k  4.000000   \n",
       "898                 manegerial   divrcd  non.white1    bach  25t39k  4.500000   \n",
       "\n",
       "     depress2   work1  comply  control  job_dich  job_disc  \n",
       "0    1.727273  psyemp       0    treat         1         4  \n",
       "1    2.000000  psyemp       0    treat         0         3  \n",
       "2    2.181818  psyump       0    treat         1         4  \n",
       "3    1.545455  psyump       0  control         0         3  \n",
       "4    2.363636  psyump       1    treat         0         2  \n",
       "..        ...     ...     ...      ...       ...       ...  \n",
       "894  1.727273  psyump       0    treat         1         3  \n",
       "895  3.636364  psyump       1    treat         0         2  \n",
       "896  1.454545  psyump       0  control         1         4  \n",
       "897  1.545455  psyemp       0  control         1         3  \n",
       "898  1.363636  psyemp       0  control         1         4  \n",
       "\n",
       "[899 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs\n",
    "# treatment column is called 'treat' \n",
    "# an outcome of interest is 'job_seek', which measures the level of job-search self-efficacy\n",
    "#    with values from 1 to 5\n",
    "# some standard covariates are sex, age, marital, nonwhite, educ, and income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = jobs[['treat', 'sex', 'age', 'depress1', 'econ_hard']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError() #specify the loss\n",
    "\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "\n",
    "sgd_callbacks = [\n",
    "        keras.callbacks.TerminateOnNaN(),\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, min_delta=0.), \n",
    "        #40 is Shi's recommendation for this dataset, but you should tune for your data \n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0., cooldown=0, min_lr=0),\n",
    "    ]\n",
    "#optimzier hyperparameters\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0003906250058207661.\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.00019531250291038305.\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 9.765625145519152e-05.\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 4.882812572759576e-05.\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 2.441406286379788e-05.\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Done\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 0.0003906250058207661.\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 0.00019531250291038305.\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 9.765625145519152e-05.\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 4.882812572759576e-05.\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.441406286379788e-05.\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.220703143189894e-05.\n",
      "\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 6.10351571594947e-06.\n",
      "29/29 [==============================] - 0s 4ms/step\n",
      "Done\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0003906250058207661.\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 0.00019531250291038305.\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 9.765625145519152e-05.\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.882812572759576e-05.\n",
      "29/29 [==============================] - 0s 9ms/step\n",
      "Done\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 7ms/step\n",
      "Done\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Done\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Done\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "29/29 [==============================] - 0s 6ms/step\n",
      "Done\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Done\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "29/29 [==============================] - 0s 10ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 4ms/step\n",
      "Done\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "29/29 [==============================] - 0s 4ms/step\n",
      "Done\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Done\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "29/29 [==============================] - 0s 4ms/step\n",
      "Done\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "29/29 [==============================] - 1s 15ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 8ms/step\n",
      "Done\n",
      "29/29 [==============================] - 1s 3ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 6ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Done\n",
      "29/29 [==============================] - 0s 12ms/step\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "sgd_lr_test = [1e-1, 1e-3, 1e-5, 1e-7, 1e-9]\n",
    "momentum_test = [0.75, 0.9]\n",
    "dims_test = [100, 200, 400, 500, 900]\n",
    "\n",
    "losses = []\n",
    "\n",
    "for lr in sgd_lr_test:\n",
    "    #for momentum in momentum_test:\n",
    "    for dim in dims_test:\n",
    "                    \n",
    "        s_learner = S_learner(dim)\n",
    "\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError() #specify the loss\n",
    "\n",
    "        s_learner.compile(optimizer= keras.optimizers.Adam(learning_rate=lr),#, nesterov=True),\n",
    "                loss=loss_fn,\n",
    "                metrics=loss_fn)\n",
    "\n",
    "        #'ys' transformed y\n",
    "        s_learner.fit(x=xt,y=jobs['job_seek'],\n",
    "                        validation_split=.2,\n",
    "                        epochs=300,\n",
    "                        batch_size=64,\n",
    "                        callbacks=sgd_callbacks,\n",
    "                        verbose=0)\n",
    "\n",
    "        loss = s_learner.loss(jobs['job_seek'], s_learner.predict(xt)).numpy()\n",
    "\n",
    "        losses.append([lr, dim, loss])\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.1, 0.75, 100, nan],\n",
       " [0.1, 0.75, 200, nan],\n",
       " [0.1, 0.9, 100, nan],\n",
       " [0.1, 0.9, 200, nan],\n",
       " [0.001, 0.75, 100, 0.5393616],\n",
       " [0.001, 0.75, 200, nan],\n",
       " [0.001, 0.9, 100, nan],\n",
       " [0.001, 0.9, 200, nan],\n",
       " [1e-05, 0.75, 100, 0.54831076],\n",
       " [1e-05, 0.75, 200, 0.54383457],\n",
       " [1e-05, 0.9, 100, 0.54781026],\n",
       " [1e-05, 0.9, 200, 0.5494427],\n",
       " [1e-07, 0.75, 100, 0.7556977],\n",
       " [1e-07, 0.75, 200, 1.1327391],\n",
       " [1e-07, 0.9, 100, 1.1106572],\n",
       " [1e-07, 0.9, 200, 0.8156351],\n",
       " [1e-09, 0.75, 100, 56.58402],\n",
       " [1e-09, 0.75, 200, 11.450971],\n",
       " [1e-09, 0.9, 100, 6.7916665],\n",
       " [1e-09, 0.9, 200, 5.134691]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses # for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.1, 100, 0.5300556],\n",
       " [0.1, 200, 1.1959561],\n",
       " [0.1, 400, 0.5300973],\n",
       " [0.1, 500, 0.5300699],\n",
       " [0.1, 900, 13.299489],\n",
       " [0.001, 100, 0.5620591],\n",
       " [0.001, 200, 0.5618848],\n",
       " [0.001, 400, 0.5567285],\n",
       " [0.001, 500, 0.5415936],\n",
       " [0.001, 900, 0.53746504],\n",
       " [1e-05, 100, 0.54704165],\n",
       " [1e-05, 200, 0.5496462],\n",
       " [1e-05, 400, 0.544023],\n",
       " [1e-05, 500, 0.54978573],\n",
       " [1e-05, 900, 0.55109537],\n",
       " [1e-07, 100, 1.2805567],\n",
       " [1e-07, 200, 3.696349],\n",
       " [1e-07, 400, 0.80296296],\n",
       " [1e-07, 500, 0.6333041],\n",
       " [1e-07, 900, 0.6464903],\n",
       " [1e-09, 100, 11.147198],\n",
       " [1e-09, 200, 14.985779],\n",
       " [1e-09, 400, 22.19937],\n",
       " [1e-09, 500, 13.275995],\n",
       " [1e-09, 900, 27.632545]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses # for Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jobs[['sex', 'age', 'depress1', 'econ_hard']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 12ms/step\n",
      "29/29 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "#create fake ones and zeros to feed network\n",
    "zeros=np.expand_dims(np.zeros(x.shape[0]),1)\n",
    "ones=np.expand_dims(np.ones(x.shape[0]),1)\n",
    "x_untreated = np.concatenate([x, zeros], 1)\n",
    "x_treated = np.concatenate([x, ones], 1)\n",
    "y0_pred_slearner=s_learner.predict(x_untreated)\n",
    "y1_pred_slearner=s_learner.predict(x_treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.546997"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_learner.loss(jobs['job_seek'], s_learner.predict(xt)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.023824032>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean((y1_pred_slearner-y0_pred_slearner).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xt, jobs['job_seek'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06012222,  0.02406536,  0.00413283, -0.26331541,  0.10260872])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>depress1</th>\n",
       "      <th>econ_hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.167122</td>\n",
       "      <td>1.91</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.101370</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.021919</td>\n",
       "      <td>2.09</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.487671</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31.610958</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.205479</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.972603</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48.909588</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.043835</td>\n",
       "      <td>1.91</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.975342</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>899 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     treat  sex        age  depress1  econ_hard\n",
       "0        1    1  34.167122      1.91       3.00\n",
       "1        1    0  26.101370      1.36       3.67\n",
       "2        1    1  35.021919      2.09       4.00\n",
       "3        0    0  27.487671      1.45       2.33\n",
       "4        1    1  31.610958      1.73       1.33\n",
       "..     ...  ...        ...       ...        ...\n",
       "894      1    0  38.205479      1.36       3.00\n",
       "895      1    0  40.972603      2.55       4.00\n",
       "896      0    1  48.909588      1.00       5.00\n",
       "897      0    1  25.043835      1.91       4.33\n",
       "898      0    1  30.975342      1.36       4.00\n",
       "\n",
       "[899 rows x 5 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae37c5a7e5a9429a2014284b76f5a225c921630615d0e6c55e705bd3d9d85b1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
